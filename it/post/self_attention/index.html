<!DOCTYPE html>
<html
  dir="ltr"
  lang="it"
  data-theme=""
  
    class="html theme--light"
  
><head>
  <title>
    
      
        Implement the self-attention mechanism in PyTorch |
      Lorenzo Balzani

  </title>

  
  <meta charset="utf-8" /><meta name="generator" content="Hugo 0.119.0"><meta name="viewport" content="width=device-width,initial-scale=1,viewport-fit=cover" />
  <meta name="author" content="Lorenzo Balzani" />
  <meta
    name="description"
    content="Python implementation of the self (scaled-dot product) attention mechanism originally proposed in &#34;Attention Is All You Need&#34;."
  />
  
    <meta name="google-site-verification" content="vu8FC47qAuUJH6xhZubBHWRBi2P5WUx_pRtgTUi1Tzw" />
  
    
    
    <link
      rel="stylesheet"
      href="/scss/main.min.1147aa5bacb4bce677a0e264073829caedb82fd18ea07a5f1d80521f539d1c45.css"
      integrity="sha256-EUeqW6y0vOZ3oOJkBzgpyu24L9GOoHpfHYBSH1OdHEU="
      crossorigin="anonymous"
      type="text/css"
    />
  

  
  <link
    rel="stylesheet"
    href="/css/markupHighlight.min.73ccfdf28df555e11009c13c20ced067af3cb021504cba43644c705930428b00.css"
    integrity="sha256-c8z98o31VeEQCcE8IM7QZ688sCFQTLpDZExwWTBCiwA="
    crossorigin="anonymous"
    type="text/css"
  />
  
  
  <link
    rel="stylesheet"
    href="/fontawesome/css/fontawesome.min.7d272de35b410fb165377550cdf9c4d3a80fbbcc961e111914e4d5c0eaf5729f.css"
    integrity="sha256-fSct41tBD7FlN3VQzfnE06gPu8yWHhEZFOTVwOr1cp8="
    crossorigin="anonymous"
    type="text/css"
  />
  
  <link
    rel="stylesheet"
    href="/fontawesome/css/solid.min.55d8333481b07a08e07cf6f37319753a2b47e99f4c395394c5747b48b495aa9b.css"
    integrity="sha256-VdgzNIGwegjgfPbzcxl1OitH6Z9MOVOUxXR7SLSVqps="
    crossorigin="anonymous"
    type="text/css"
  />
  
  <link
    rel="stylesheet"
    href="/fontawesome/css/regular.min.a7448d02590b43449364b6b5922ed9af5410abb4de4238412a830316dedb850b.css"
    integrity="sha256-p0SNAlkLQ0STZLa1ki7Zr1QQq7TeQjhBKoMDFt7bhQs="
    crossorigin="anonymous"
    type="text/css"
  />
  
  <link
    rel="stylesheet"
    href="/fontawesome/css/brands.min.9ed75a5d670c953fe4df935937674b4646f92674367e9e66eb995bb04e821647.css"
    integrity="sha256-ntdaXWcMlT/k35NZN2dLRkb5JnQ2fp5m65lbsE6CFkc="
    crossorigin="anonymous"
    type="text/css"
  />
  
  <link rel="shortcut icon" href="/favicons/favicon.ico" type="image/x-icon" />
  <link rel="apple-touch-icon" sizes="180x180" href="/favicons/apple-touch-icon.png" />
  <link rel="icon" type="image/png" sizes="32x32" href="/favicons/favicon-32x32.png" />
  <link rel="icon" type="image/png" sizes="16x16" href="/favicons/favicon-16x16.png" />

  <link rel="canonical" href="https://lorenzobalzani.github.io/it/post/self_attention/" />

  
  
  
  
  <script
    type="text/javascript"
    src="/js/anatole-header.min.f9132794301a01ff16550ed66763482bd848f62243d278f5e550229a158bfd32.js"
    integrity="sha256-&#43;RMnlDAaAf8WVQ7WZ2NIK9hI9iJD0nj15VAimhWL/TI="
    crossorigin="anonymous"
  ></script>

  
    
    
    <script
      type="text/javascript"
      src="/js/anatole-theme-switcher.min.d6d329d93844b162e8bed1e915619625ca91687952177552b9b3e211014a2957.js"
      integrity="sha256-1tMp2ThEsWLovtHpFWGWJcqRaHlSF3VSubPiEQFKKVc="
      crossorigin="anonymous"
    ></script>
  

  


  
  <meta name="twitter:card" content="summary_large_image"/>
<meta name="twitter:image" content="https://lorenzobalzani.github.io/images/wallpaper.png"/>

<meta name="twitter:title" content="Implement the self-attention mechanism in PyTorch"/>
<meta name="twitter:description" content="Python implementation of the self (scaled-dot product) attention mechanism originally proposed in &#34;Attention Is All You Need&#34;."/>



  
  <meta property="og:title" content="Implement the self-attention mechanism in PyTorch" />
<meta property="og:description" content="Python implementation of the self (scaled-dot product) attention mechanism originally proposed in &#34;Attention Is All You Need&#34;." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://lorenzobalzani.github.io/it/post/self_attention/" /><meta property="og:image" content="https://lorenzobalzani.github.io/images/wallpaper.png"/><meta property="article:section" content="post" />
<meta property="article:published_time" content="2023-03-10T00:00:00+00:00" />
<meta property="article:modified_time" content="2023-03-10T00:00:00+00:00" /><meta property="og:site_name" content="I&#39;m Lorenzo Balzani" />




  
  
  
  
  <script type="application/ld+json">
    {
        "@context": "http://schema.org",
        "@type": "BlogPosting",
        "articleSection": "post",
        "name": "Implement the self-attention mechanism in PyTorch",
        "headline": "Implement the self-attention mechanism in PyTorch",
        "alternativeHeadline": "",
        "description": "
      Python implementation of the self (scaled-dot product) attention mechanism originally proposed in \u0022Attention Is All You Need\u0022.


    ",
        "inLanguage": "it",
        "isFamilyFriendly": "true",
        "mainEntityOfPage": {
            "@type": "WebPage",
            "@id": "https:\/\/lorenzobalzani.github.io\/it\/post\/self_attention\/"
        },
        "author" : {
            "@type": "Person",
            "name": "Lorenzo Balzani"
        },
        "creator" : {
            "@type": "Person",
            "name": "Lorenzo Balzani"
        },
        "accountablePerson" : {
            "@type": "Person",
            "name": "Lorenzo Balzani"
        },
        "copyrightHolder" : {
            "@type": "Person",
            "name": "Lorenzo Balzani"
        },
        "copyrightYear" : "2023",
        "dateCreated": "2023-03-10T00:00:00.00Z",
        "datePublished": "2023-03-10T00:00:00.00Z",
        "dateModified": "2023-03-10T00:00:00.00Z",
        "publisher":{
            "@type":"Organization",
            "name": "Lorenzo Balzani",
            "url": "https://lorenzobalzani.github.io/",
            "logo": {
                "@type": "ImageObject",
                "url": "https:\/\/lorenzobalzani.github.io\/favicons\/favicon-32x32.png",
                "width":"32",
                "height":"32"
            }
        },
        "image": 
      [
        
        "https://lorenzobalzani.github.io/images/wallpaper.png"


      
      ]

    ,
        "url" : "https:\/\/lorenzobalzani.github.io\/it\/post\/self_attention\/",
        "wordCount" : "3064",
        "genre" : [ ],
        "keywords" : [ 
      
      "attention"

    
      
        ,

      
      "nlp"

    
      
        ,

      
      "deep learning"

    ]
    }
  </script>


</head>
<body class="body">
    <div class="wrapper">
      <aside
        
          class="wrapper__sidebar"
        
      ><div
  class="sidebar
    animated fadeInDown
  "
>
  <div class="sidebar__content">
    <div class="sidebar__introduction">
      <img
        class="sidebar__introduction-profileimage"
        src="/images/profile.png"
        alt="profile picture"
      />
      
        <div class="sidebar__introduction-title">
          <a href="/it">I&#39;m Lorenzo Balzani</a>
        </div>
      
      <div class="sidebar__introduction-description">
        <p>NLP Data Scientist @ Prometeia<br />Studente magistrale in AI @ Unibo<br />Laureato in Computer Science and Engineering @ Unibo</p>
      </div>
    </div>
    <ul class="sidebar__list">
      
        <li class="sidebar__list-item">
          <a
            href="https://www.linkedin.com/in/lorenzobalzani/"
            target="_blank"
            rel="noopener me"
            aria-label="Linkedin"
            title="Linkedin"
          >
            <i class="fab fa-linkedin fa-2x" aria-hidden="true"></i>
          </a>
        </li>
      
        <li class="sidebar__list-item">
          <a
            href="https://github.com/lorenzobalzani"
            target="_blank"
            rel="noopener me"
            aria-label="GitHub"
            title="GitHub"
          >
            <i class="fab fa-github fa-2x" aria-hidden="true"></i>
          </a>
        </li>
      
        <li class="sidebar__list-item">
          <a
            href="mailto:balzanilo@icloud.com"
            target="_blank"
            rel="noopener me"
            aria-label="e-mail"
            title="e-mail"
          >
            <i class="fas fa-envelope fa-2x" aria-hidden="true"></i>
          </a>
        </li>
      
    </ul>
  </div><footer class="footer footer__sidebar">
  <ul class="footer__list">
    <li class="footer__item">
      &copy;
      
        Lorenzo Balzani
        2024
      
    </li>
    
  </ul>
</footer>
  
  <script
    type="text/javascript"
    src="/js/medium-zoom.min.1248fa75275e5ef0cbef27e8c1e27dc507c445ae3a2c7d2ed0be0809555dac64.js"
    integrity="sha256-Ekj6dSdeXvDL7yfoweJ9xQfERa46LH0u0L4ICVVdrGQ="
    crossorigin="anonymous"
  ></script><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.13.0/dist/katex.min.css" integrity="sha384-t5CR&#43;zwDAROtph0PXGte6ia8heboACF9R5l/DiY&#43;WZ3P2lxNgvJkQk5n7GPvLMYw" crossorigin="anonymous" /><script defer src="https://cdn.jsdelivr.net/npm/katex@0.13.0/dist/katex.min.js" integrity="sha384-FaFLTlohFghEIZkw6VGwmf9ISTubWAVYW8tG8&#43;w2LAIftJEULZABrF9PPFv&#43;tVkH" crossorigin="anonymous"></script><script
      defer
      src="https://cdn.jsdelivr.net/npm/katex@0.13.0/dist/contrib/auto-render.min.js"
      integrity="sha384-bHBqxz8fokvgoJ/sc17HODNxa42TlaEhB&#43;w8ZJXTc2nZf1VgEaFZeZvT4Mznfz0v"
      crossorigin="anonymous"
      onload="renderMathInElement(document.body);"
    ></script></div>
</aside>
      <main
        
          class="wrapper__main"
        
      >
        <header class="header"><div
  class="
    animated fadeInDown
  "
>
  <a role="button" class="navbar-burger" data-target="navMenu" aria-label="menu" aria-expanded="false">
    <span aria-hidden="true" class="navbar-burger__line"></span>
    <span aria-hidden="true" class="navbar-burger__line"></span>
    <span aria-hidden="true" class="navbar-burger__line"></span>
  </a>
  <nav class="nav">
    <ul class="nav__list" id="navMenu">
      
      
        
        
          <li class="nav__list-item">
            
            <a
              
              href="/"
              
              title=""
              >Home</a
            >
          </li>
        

      
        
        
          <li class="nav__list-item">
            
            <a
              
              href="/post/"
              
              title=""
              >Articoli</a
            >
          </li>
        

      
        
        
          <li class="nav__list-item">
            
            <a
              
              href="/portfolio/"
              
              title=""
              >Portfolio</a
            >
          </li>
        

      
        
        
          <li class="nav__list-item">
            
            <a
              
              href="/resume/"
              
              title=""
              >Resume</a
            >
          </li>
        

      
        
        
          <li class="nav__list-item">
            
            <a
              
              href="/contact/"
              
              title=""
              >Contatti</a
            >
          </li>
        

      
    </ul>
    <ul class="nav__list nav__list--end">
      
        <li class="nav__list-item">
          <div class="optionswitch">
            <input class="optionswitch__picker" type="checkbox" id="languagepicker" hidden />
            <label class="optionswitch__label" for="languagepicker"
              >IT <i class="fa fa-angle-down" aria-hidden="true"></i
            ></label>
            <div class="optionswitch__triangle"></div>
            <ul class="optionswitch__list">
              
                <li class="optionswitch__list-item">
                  <a href="/post/self_attention/" title="EN"
                    ><span
                      
                      aria-label="EN"
                      >EN</span
                    >
                  </a>
                </li>
              
            </ul>
          </div>
        </li>
      
      
        <li class="nav__list-item">
          <div class="themeswitch">
            <a title="Switch Theme">
              <i class="fas fa-adjust fa-fw" aria-hidden="true"></i>
            </a>
          </div>
        </li>
      
    </ul>
  </nav>
</div>
</header>
  <div
    class="post 
      animated fadeInDown
    "
  >
    
    <div class="post__content">
      <h1>Implement the Self-Attention Mechanism in PyTorch</h1>
      
        <ul class="post__meta">
          <li class="post__meta-item">
            <em class="fas fa-calendar-day post__meta-icon"></em>
            <span class="post__meta-text"
              >
                
                  10/3/2023
                

              
            </span>
          </li>
          <li class="post__meta-item">
            <em class="fas fa-stopwatch post__meta-icon"></em>
            <span class="post__meta-text">Tempo di lettura: 15 minuti.</span>
          </li>
        </ul>
      <p>Python implementation of the self (scaled-dot product) attention mechanism originally proposed in &ldquo;Attention Is All You Need&rdquo;. Note that this is intended to be an executable and extended version of <a href="https://sebastianraschka.com/blog/2023/self-attention-from-scratch.html">this article</a>.</p>
<p>Would you like to download the code? Check out this <a href="https://github.com/lorenzobalzani/nlp-dl-experiments">GitHub repository</a>.</p>
<h1 id="setup">Setup</h1>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="kn">import</span> <span class="nn">torch</span>
</span></span><span class="line"><span class="cl"><span class="kn">import</span> <span class="nn">torch.nn.functional</span> <span class="k">as</span> <span class="nn">F</span>
</span></span></code></pre></div><div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="n">torch</span><span class="o">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">dim_word_embedding</span> <span class="o">=</span> <span class="mi">16</span>
</span></span></code></pre></div><p>Let&rsquo;s map each <strong>unique</strong> word present in the sentence to an index.
In real-world scenarios, there would be a bigger vocubalary, e.g. hundred of thousands of words.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="n">sentence</span> <span class="o">=</span> <span class="s1">&#39;Life is short, eat dessert first is life&#39;</span>
</span></span><span class="line"><span class="cl"><span class="n">sentence</span> <span class="o">=</span> <span class="n">sentence</span><span class="o">.</span><span class="n">lower</span><span class="p">()</span><span class="o">.</span><span class="n">replace</span><span class="p">(</span><span class="s1">&#39;,&#39;</span><span class="p">,</span> <span class="s1">&#39;&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">split</span><span class="p">()</span>
</span></span><span class="line"><span class="cl"><span class="n">n_sentence</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">sentence</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">dict_words</span> <span class="o">=</span> <span class="p">{</span><span class="n">word</span><span class="p">:</span> <span class="n">idx</span> <span class="k">for</span> <span class="n">idx</span><span class="p">,</span> <span class="n">word</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="nb">sorted</span><span class="p">(</span><span class="n">sentence</span><span class="p">))}</span>
</span></span><span class="line"><span class="cl"><span class="n">sentence_ints</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="n">dict_words</span><span class="p">[</span><span class="n">char</span><span class="p">]</span> <span class="k">for</span> <span class="n">char</span> <span class="ow">in</span> <span class="n">sentence</span><span class="p">])</span>
</span></span></code></pre></div><div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Word dictionary =&#39;</span><span class="p">,</span> <span class="n">dict_words</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Numeric representation of the input sentence =&#39;</span><span class="p">,</span> <span class="n">sentence_ints</span><span class="p">)</span>
</span></span></code></pre></div><pre><code>Word dictionary = {'dessert': 0, 'eat': 1, 'first': 2, 'is': 4, 'life': 6, 'short': 7}
Numeric representation of the input sentence = tensor([6, 4, 7, 1, 0, 2, 4, 6])
</code></pre>
<h1 id="single-head-self-attention">Single head self-attention</h1>
<figure class="medium"><img src="/images/self_attention/single-head.png"
         alt="image"/><figcaption>
            <p>A single self-attention head</p>
        </figcaption>
</figure>

<p>Now create a \(n_{words} \cdot dim_{word_{embedding}} = 8 \cdot 16\) embedding tensor.</p>
<p>These initial values will be randomly generated and \(\dim_{vec} = 16\) is an hyper-parameter.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="n">embedding_layer</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Embedding</span><span class="p">(</span><span class="n">n_sentence</span><span class="p">,</span> <span class="n">dim_word_embedding</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">embedded_sentence</span> <span class="o">=</span> <span class="n">embedding_layer</span><span class="p">(</span><span class="n">sentence_ints</span><span class="p">)</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span>
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Embedded sentence shape =&#39;</span><span class="p">,</span> <span class="n">embedded_sentence</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
</span></span></code></pre></div><pre><code>Embedded sentence shape = torch.Size([8, 16])
</code></pre>
<p>Let&rsquo;s choose values for Queries, Keys, and Values.
Since Queries and Keys vector are multiplied afterward, their dimensions MUST be equal.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="n">dim_q</span><span class="p">,</span> <span class="n">dim_v</span> <span class="o">=</span> <span class="mi">24</span><span class="p">,</span> <span class="mi">28</span>
</span></span><span class="line"><span class="cl"><span class="n">dim_k</span> <span class="o">=</span> <span class="n">dim_q</span>
</span></span><span class="line"><span class="cl"><span class="n">W_q</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="n">dim_q</span><span class="p">,</span> <span class="n">dim_word_embedding</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">W_k</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="n">dim_k</span><span class="p">,</span> <span class="n">dim_word_embedding</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">W_v</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="n">dim_v</span><span class="p">,</span> <span class="n">dim_word_embedding</span><span class="p">)</span>
</span></span></code></pre></div><h2 id="compute-queries-keys-values-for-the-input-words">Compute queries, keys, values for the input words</h2>
<p>First, try to understand this on single words.</p>
<h3 id="example-1">Example #1</h3>
<p>Compute query, key, and value vector related to the first and second words in the sentence (\(idx = 0, 1\)).</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="n">word_idx</span> <span class="o">=</span> <span class="mi">0</span>
</span></span><span class="line"><span class="cl"><span class="n">x_1</span> <span class="o">=</span> <span class="n">embedded_sentence</span><span class="p">[</span><span class="n">word_idx</span><span class="p">]</span>
</span></span><span class="line"><span class="cl"><span class="n">query_1</span> <span class="o">=</span> <span class="n">W_q</span> <span class="o">@</span> <span class="n">x_1</span>
</span></span><span class="line"><span class="cl"><span class="n">key_1</span> <span class="o">=</span> <span class="n">W_k</span> <span class="o">@</span> <span class="n">x_1</span>
</span></span><span class="line"><span class="cl"><span class="n">value_1</span> <span class="o">=</span> <span class="n">W_v</span> <span class="o">@</span> <span class="n">x_1</span>
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Word idx =&#39;</span><span class="p">,</span> <span class="n">word_idx</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="s1">&#39;</span><span class="se">\t</span><span class="s1">Query size =&#39;</span><span class="p">,</span> <span class="n">query_1</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="s1">&#39;</span><span class="se">\t</span><span class="s1">Key size =&#39;</span><span class="p">,</span> <span class="n">key_1</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="s1">&#39;</span><span class="se">\t</span><span class="s1">Value size =&#39;</span><span class="p">,</span> <span class="n">value_1</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
</span></span></code></pre></div><pre><code>Word idx = 0
	Query size = torch.Size([24])
	Key size = torch.Size([24])
	Value size = torch.Size([28])
</code></pre>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="n">word_idx</span> <span class="o">=</span> <span class="mi">1</span>
</span></span><span class="line"><span class="cl"><span class="n">x_2</span> <span class="o">=</span> <span class="n">embedded_sentence</span><span class="p">[</span><span class="n">word_idx</span><span class="p">]</span>
</span></span><span class="line"><span class="cl"><span class="n">query_2</span> <span class="o">=</span> <span class="n">W_q</span> <span class="o">@</span> <span class="n">x_2</span>
</span></span><span class="line"><span class="cl"><span class="n">key_2</span> <span class="o">=</span> <span class="n">W_k</span> <span class="o">@</span> <span class="n">x_2</span>
</span></span><span class="line"><span class="cl"><span class="n">value_2</span> <span class="o">=</span> <span class="n">W_v</span> <span class="o">@</span> <span class="n">x_2</span>
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Word idx =&#39;</span><span class="p">,</span> <span class="n">word_idx</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="s1">&#39;</span><span class="se">\t</span><span class="s1">Query size =&#39;</span><span class="p">,</span> <span class="n">query_2</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="s1">&#39;</span><span class="se">\t</span><span class="s1">Key size =&#39;</span><span class="p">,</span> <span class="n">key_2</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="s1">&#39;</span><span class="se">\t</span><span class="s1">Value size =&#39;</span><span class="p">,</span> <span class="n">value_2</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
</span></span></code></pre></div><pre><code>Word idx = 1
	Query size = torch.Size([24])
	Key size = torch.Size([24])
	Value size = torch.Size([28])
</code></pre>
<h3 id="compute-queries-keys-values-for-all-input-words">Compute queries, keys, values for ALL input words</h3>
<p>Now that I have understood the process, let&rsquo;s extend this to all the input words.
Transpose the matrices to have words on rows and vector components on columns.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="n">queries</span> <span class="o">=</span> <span class="p">(</span><span class="n">W_q</span> <span class="o">@</span> <span class="n">embedded_sentence</span><span class="o">.</span><span class="n">T</span><span class="p">)</span><span class="o">.</span><span class="n">T</span>
</span></span><span class="line"><span class="cl"><span class="n">keys</span> <span class="o">=</span> <span class="p">(</span><span class="n">W_k</span> <span class="o">@</span> <span class="n">embedded_sentence</span><span class="o">.</span><span class="n">T</span><span class="p">)</span><span class="o">.</span><span class="n">T</span>
</span></span><span class="line"><span class="cl"><span class="n">values</span> <span class="o">=</span> <span class="p">(</span><span class="n">W_v</span> <span class="o">@</span> <span class="n">embedded_sentence</span><span class="o">.</span><span class="n">T</span><span class="p">)</span><span class="o">.</span><span class="n">T</span>
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="s1">&#39;All input words -&gt; queries.shape =&#39;</span><span class="p">,</span> <span class="n">queries</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span> 
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="s1">&#39;All input words -&gt; keys.shape =&#39;</span><span class="p">,</span> <span class="n">keys</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span> 
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="s1">&#39;All input words -&gt; values.shape =&#39;</span><span class="p">,</span> <span class="n">values</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
</span></span></code></pre></div><pre><code>All input words -&gt; queries.shape = torch.Size([8, 24])
All input words -&gt; keys.shape = torch.Size([8, 24])
All input words -&gt; values.shape = torch.Size([8, 28])
</code></pre>
<h2 id="unnormalized-attention-scores">Unnormalized attention scores</h2>
<p>As before, I am trying to understand this concept on single words.</p>
<h3 id="example-2">Example #2</h3>
<p>Let&rsquo;s compute the unnormalized attention score \(\omega\) (omega) for the first word w.r.t. the 5th word</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="n">omega_1_5</span> <span class="o">=</span> <span class="n">queries</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">@</span> <span class="n">keys</span><span class="p">[</span><span class="mi">4</span><span class="p">]</span>
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Unnormalized attention score of first word w.r.t. 5th word =&#39;</span><span class="p">,</span> <span class="n">omega_1_5</span><span class="o">.</span><span class="n">item</span><span class="p">())</span>
</span></span></code></pre></div><pre><code>Unnormalized attention score of first word w.r.t. 5th word = -46.02456283569336
</code></pre>
<h3 id="compute-unnormalized-attention-scores-wrt-all-input-words">Compute unnormalized attention scores w.r.t. ALL input words</h3>
<p>Let&rsquo;s compute the unnormalized attention scores for the first word w.r.t. to all other words.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="n">omega_1_all</span> <span class="o">=</span> <span class="n">queries</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">@</span> <span class="n">keys</span><span class="o">.</span><span class="n">T</span>
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;Unnormalized attention scores of first word w.r.t. ALL other words:</span><span class="se">\n</span><span class="si">{</span><span class="n">omega_1_all</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
</span></span></code></pre></div><pre><code>Unnormalized attention scores of first word w.r.t. ALL other words:
tensor([ 47.9667,  58.9805,  42.1271, 141.0643, -46.0246, -72.1767,  58.9805,
         47.9667])
</code></pre>
<h3 id="compute-unnormalized-attention-scores-for-all-input-words">Compute unnormalized attention scores for ALL input words</h3>
<p>Let&rsquo;s compute the unnormalized attention scores for all the words w.r.t. to all other words.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="n">omega_all</span> <span class="o">=</span> <span class="n">queries</span> <span class="o">@</span> <span class="n">keys</span><span class="o">.</span><span class="n">T</span>
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;Unnormalized attention scores of ALL words w.r.t. ALL other words:</span><span class="se">\n</span><span class="si">{</span><span class="n">omega_all</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="s2">&#34;All input words scores -&gt; omega_all.shape =&#34;</span><span class="p">,</span> <span class="n">omega_all</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span> 
</span></span></code></pre></div><pre><code>Unnormalized attention scores of ALL words w.r.t. ALL other words:
tensor([[  47.9667,   58.9805,   42.1272,  141.0642,  -46.0246,  -72.1767,
           58.9805,   47.9667],
        [  58.7503,   93.6661,   65.4516,  229.0244,  -51.6797, -109.5712,
           93.6661,   58.7503],
        [  47.7602,   53.6036,   42.4971,  132.3753,  -45.4809,  -63.6152,
           53.6036,   47.7602],
        [ 145.1907,  182.7591,  157.4097,  479.7147, -139.6413, -238.2749,
          182.7591,  145.1907],
        [ -26.0050,  -25.1779,  -21.8697,  -61.3257,   20.7895,   35.2284,
          -25.1779,  -26.0050],
        [ -71.2604,  -94.5636,  -83.3776, -257.1654,   66.7258,  130.5431,
          -94.5636,  -71.2604],
        [  58.7503,   93.6661,   65.4516,  229.0244,  -51.6797, -109.5712,
           93.6661,   58.7503],
        [  47.9667,   58.9805,   42.1272,  141.0642,  -46.0246,  -72.1767,
           58.9805,   47.9667]])
All input words scores -&gt; omega_all.shape = torch.Size([8, 8])
</code></pre>
<h2 id="normalized-attention-scores">Normalized attention scores</h2>
<p>Why? To reach more numeric stability and thus, reduce errors.
Note that the sum of the values is 1 (thanks to the softmax function).</p>
<h3 id="example-3">Example #3</h3>
<p>Let&rsquo;s compute the normalized attention score \(\alpha\) (alpha) for the first word w.r.t. the ALL other words.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="n">normalized_attention_scores_1</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">omega_1_all</span> <span class="o">/</span> <span class="n">dim_k</span> <span class="o">**</span> <span class="mf">0.5</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;Normalized attention scores of first word w.r.t. ALL other words:</span><span class="se">\n</span><span class="si">{</span><span class="n">normalized_attention_scores_1</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Sum of this vector =&#39;</span><span class="p">,</span> <span class="n">normalized_attention_scores_1</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span><span class="o">.</span><span class="n">item</span><span class="p">())</span>
</span></span></code></pre></div><pre><code>Normalized attention scores of first word w.r.t. ALL other words:
tensor([5.5834e-09, 5.2879e-08, 1.6952e-09, 1.0000e+00, 2.5976e-17, 1.2479e-19,
        5.2879e-08, 5.5834e-09])
Sum of this vector = 1.0
</code></pre>
<h3 id="compute-normalized-attention-scores-for-all-input-words">Compute normalized attention scores for ALL input words</h3>
<p>Let&rsquo;s compute the normalized attention score \(\alpha\) (alpha) for ALL words w.r.t. ALL other words.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="n">normalized_attention_scores</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">omega_all</span> <span class="o">/</span> <span class="n">dim_k</span> <span class="o">**</span> <span class="mf">0.5</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;Normalized attention scores of ALL words w.r.t. ALL other words:</span><span class="se">\n</span><span class="si">{</span><span class="n">normalized_attention_scores</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Sum of this vector =&#39;</span><span class="p">,</span> <span class="n">normalized_attention_scores</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span><span class="o">.</span><span class="n">item</span><span class="p">())</span>
</span></span></code></pre></div><pre><code>Normalized attention scores of ALL words w.r.t. ALL other words:
tensor([[2.4049e-09, 1.0642e-11, 6.0284e-11, 9.5202e-31, 1.0108e-10, 1.0688e-18,
         1.0642e-11, 2.4049e-09],
        [2.1730e-08, 1.2645e-08, 7.0456e-09, 5.9746e-23, 3.1866e-11, 5.1745e-22,
         1.2645e-08, 2.1730e-08],
        [2.3056e-09, 3.5511e-12, 6.5013e-11, 1.6157e-31, 1.1294e-10, 6.1358e-18,
         3.5511e-12, 2.3056e-09],
        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 5.0760e-19, 2.0151e-33,
         1.0000e+00, 1.0000e+00],
        [6.6606e-16, 3.6846e-19, 1.2790e-16, 0.0000e+00, 8.4667e-05, 3.5510e-09,
         3.6846e-19, 6.6606e-16],
        [6.4806e-20, 2.6022e-25, 4.5103e-22, 0.0000e+00, 9.9992e-01, 1.0000e+00,
         2.6022e-25, 6.4806e-20],
        [2.1730e-08, 1.2645e-08, 7.0456e-09, 5.9747e-23, 3.1866e-11, 5.1745e-22,
         1.2645e-08, 2.1730e-08],
        [2.4049e-09, 1.0642e-11, 6.0284e-11, 9.5202e-31, 1.0108e-10, 1.0688e-18,
         1.0642e-11, 2.4049e-09]])
Sum of this vector = 8.0
</code></pre>
<p>##Â Compute \(z\) vector
The \(z\) vector is an enhanced version of the input word. In other words, it <em>&ldquo;embeds&rdquo;</em> the information about ALL the other inputs words.</p>
<figure class="medium"><img src="/images/self_attention/context-vector.png"
         alt="image"/><figcaption>
            <p>The context vector</p>
        </figcaption>
</figure>

<p>Note that the dimension of \(z = dim_v\), and in this case \(dim_v &gt; dim_{word_{embedding}}\), but it turns out that can be arbitrarly chosen.</p>
<h3 id="example-4">Example #4</h3>
<p>Let&rsquo;s compute \(z\) for the first word w.r.t. ALL other words.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="n">context_vector_z_1</span> <span class="o">=</span> <span class="n">normalized_attention_scores_1</span> <span class="o">@</span> <span class="n">values</span>
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Context vector related to the first word w.r.t. ALL other words&#39;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Shape =&#39;</span><span class="p">,</span> <span class="n">context_vector_z_1</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="n">context_vector_z_1</span><span class="p">)</span>
</span></span></code></pre></div><pre><code>Context vector related to the first word w.r.t. ALL other words
Shape = torch.Size([28])
tensor([5.2083, 4.5906, 3.1900, 4.1853, 4.7579, 4.2178, 3.4120, 5.0137, 4.0621,
        4.1455, 6.3549, 3.0836, 6.4934, 4.0425, 4.8676, 3.0821, 6.8482, 5.5784,
        4.6929, 5.4580, 5.6707, 4.9629, 4.2686, 5.6802, 5.3528, 4.5219, 4.6112,
        4.7807])
</code></pre>
<h3 id="compute-z-for-all-input-words">Compute \(z\) for ALL input words</h3>
<p>Let&rsquo;s compute \(z\) for ALL words w.r.t. ALL other words.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="n">context_z</span> <span class="o">=</span> <span class="n">normalized_attention_scores</span> <span class="o">@</span> <span class="n">values</span>
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Context vector related to ALL input words w.r.t. ALL other words&#39;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Shape =&#39;</span><span class="p">,</span> <span class="n">context_z</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="n">context_z</span><span class="p">)</span>
</span></span></code></pre></div><pre><code>Context vector related to ALL input words w.r.t. ALL other words
Shape = torch.Size([8, 28])
tensor([[ 1.0208e-08,  4.5245e-09,  8.9584e-09,  8.8031e-09,  1.0888e-08,
          1.1263e-08,  6.6825e-09,  9.1426e-09,  3.3903e-09,  9.7982e-09,
          1.1035e-08,  4.7999e-09,  1.4235e-08,  1.4572e-08,  6.5525e-09,
          7.8531e-09,  8.0995e-09,  1.5258e-08,  1.1228e-09,  1.2500e-08,
          1.6545e-09,  8.0533e-09,  1.2058e-08,  1.8843e-09,  1.4327e-08,
          1.2405e-08,  1.1857e-09,  1.0192e-08],
        [ 1.3333e-07,  1.4292e-07,  1.4621e-07,  5.8652e-08,  9.4583e-08,
          2.0076e-07,  1.0602e-07,  1.5732e-07,  9.1718e-08,  1.1024e-07,
          1.1862e-07,  5.8737e-08,  1.7114e-07,  1.5896e-07,  1.0163e-07,
          9.6130e-08,  1.6256e-07,  2.0144e-07,  6.1971e-08,  1.4980e-07,
          5.8659e-08,  1.8701e-07,  1.8030e-07,  9.1482e-08,  1.2541e-07,
          1.3888e-07,  7.5303e-08,  1.3610e-07],
        [ 9.7775e-09,  4.3164e-09,  8.5718e-09,  8.4555e-09,  1.0414e-08,
          1.0743e-08,  6.3774e-09,  8.7203e-09,  3.2221e-09,  9.3672e-09,
          1.0560e-08,  4.5993e-09,  1.3607e-08,  1.3972e-08,  6.2539e-09,
          7.5503e-09,  7.7105e-09,  1.4573e-08,  1.0270e-09,  1.1957e-08,
          1.5478e-09,  7.6659e-09,  1.1533e-08,  1.7809e-09,  1.3734e-08,
          1.1868e-08,  1.1197e-09,  9.7389e-09],
        [ 1.3486e+01,  1.5032e+01,  1.2241e+01,  6.8524e+00,  1.0054e+01,
          1.7727e+01,  1.0002e+01,  1.5045e+01,  1.0787e+01,  1.0733e+01,
          1.3517e+01,  7.1050e+00,  1.6438e+01,  1.3163e+01,  1.2065e+01,
          8.8727e+00,  1.8247e+01,  1.7620e+01,  9.5952e+00,  1.4581e+01,
          1.0128e+01,  1.7828e+01,  1.5626e+01,  1.3258e+01,  1.2085e+01,
          1.2815e+01,  1.0607e+01,  1.3737e+01],
        [-2.8815e-05,  1.2425e-04,  8.1114e-05, -6.0943e-05, -2.9221e-04,
         -1.4321e-04, -5.3928e-05, -7.0047e-05, -3.8643e-05, -1.8990e-04,
         -1.9285e-04, -7.4484e-05, -1.9050e-04, -2.8142e-05, -1.7792e-04,
          1.2690e-04, -1.6557e-04, -2.0935e-04, -1.6793e-04, -1.7158e-04,
         -1.2642e-04, -2.8246e-05, -3.0229e-05, -3.8677e-05, -1.6634e-04,
         -1.9244e-04,  4.7392e-05, -2.1519e-04],
        [-2.0627e+00, -1.8042e+00, -5.9498e-02, -3.8449e+00, -6.9198e+00,
         -2.7874e+00, -2.7707e+00, -2.4009e+00, -4.4370e+00, -5.5688e+00,
         -4.6235e+00, -2.1498e+00, -1.5360e+00, -7.4642e-01, -5.4877e+00,
         -6.5154e-01, -3.2329e+00, -3.8940e+00, -5.0657e+00, -2.5105e+00,
         -7.0453e+00, -2.9677e+00, -1.5344e+00, -2.5249e+00, -5.9545e+00,
         -2.9639e+00, -2.7855e+00, -5.6987e+00],
        [ 1.3333e-07,  1.4291e-07,  1.4621e-07,  5.8652e-08,  9.4584e-08,
          2.0076e-07,  1.0602e-07,  1.5732e-07,  9.1717e-08,  1.1024e-07,
          1.1862e-07,  5.8737e-08,  1.7114e-07,  1.5896e-07,  1.0163e-07,
          9.6130e-08,  1.6256e-07,  2.0144e-07,  6.1971e-08,  1.4980e-07,
          5.8659e-08,  1.8701e-07,  1.8030e-07,  9.1482e-08,  1.2541e-07,
          1.3888e-07,  7.5303e-08,  1.3610e-07],
        [ 1.0208e-08,  4.5245e-09,  8.9584e-09,  8.8031e-09,  1.0888e-08,
          1.1263e-08,  6.6825e-09,  9.1426e-09,  3.3903e-09,  9.7982e-09,
          1.1035e-08,  4.7999e-09,  1.4235e-08,  1.4572e-08,  6.5525e-09,
          7.8531e-09,  8.0995e-09,  1.5258e-08,  1.1228e-09,  1.2500e-08,
          1.6545e-09,  8.0533e-09,  1.2058e-08,  1.8843e-09,  1.4327e-08,
          1.2405e-08,  1.1857e-09,  1.0192e-08]])
</code></pre>
<h2 id="ok-butare-you-getting-confused-by-all-these-dimension">Ok but&hellip;are you getting confused by all these dimension?</h2>
<p>Try to check out this picture.
<figure class="medium"><img src="/images/self_attention/single-attention-head.png"
         alt="image"/><figcaption>
            <p>A single self-attention head</p>
        </figcaption>
</figure>
</p>
<h1 id="multi-head-self-attention">Multi-head self-attention</h1>
<p>Each query has now shape \(n_{heads} \times dim_{word_{embedding}}\).</p>
<figure class="medium"><img src="/images/self_attention/multi-head.png"
         alt="image"/><figcaption>
            <p>A multi self-attention head</p>
        </figcaption>
</figure>

<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="n">n_heads</span> <span class="o">=</span> <span class="mi">3</span>
</span></span><span class="line"><span class="cl"><span class="n">multihead_W_query</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="n">n_heads</span><span class="p">,</span> <span class="n">dim_q</span><span class="p">,</span> <span class="n">dim_word_embedding</span><span class="p">)</span> 
</span></span><span class="line"><span class="cl"><span class="n">multihead_W_key</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="n">n_heads</span><span class="p">,</span> <span class="n">dim_k</span><span class="p">,</span> <span class="n">dim_word_embedding</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">multihead_W_value</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="n">n_heads</span><span class="p">,</span> <span class="n">dim_v</span><span class="p">,</span> <span class="n">dim_word_embedding</span><span class="p">)</span>
</span></span></code></pre></div><p>Follow this example with the first word.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="n">word_idx</span> <span class="o">=</span> <span class="mi">0</span>
</span></span><span class="line"><span class="cl"><span class="n">x_1</span> <span class="o">=</span> <span class="n">embedded_sentence</span><span class="p">[</span><span class="n">word_idx</span><span class="p">]</span>
</span></span><span class="line"><span class="cl"><span class="n">multihead_query_1</span> <span class="o">=</span> <span class="n">multihead_W_query</span> <span class="o">@</span> <span class="n">x_1</span>
</span></span><span class="line"><span class="cl"><span class="n">multihead_key_1</span> <span class="o">=</span> <span class="n">multihead_W_key</span> <span class="o">@</span> <span class="n">x_1</span>
</span></span><span class="line"><span class="cl"><span class="n">multihead_value_1</span> <span class="o">=</span> <span class="n">multihead_W_value</span> <span class="o">@</span> <span class="n">x_1</span>
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Word idx =&#39;</span><span class="p">,</span> <span class="n">word_idx</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="s1">&#39;</span><span class="se">\t</span><span class="s1">Multihead-query size =&#39;</span><span class="p">,</span> <span class="n">multihead_query_1</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="s1">&#39;</span><span class="se">\t</span><span class="s1">Multihead-key size =&#39;</span><span class="p">,</span> <span class="n">multihead_key_1</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="s1">&#39;</span><span class="se">\t</span><span class="s1">Multihead-value size =&#39;</span><span class="p">,</span> <span class="n">multihead_value_1</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
</span></span></code></pre></div><pre><code>Word idx = 0
	Multihead-query size = torch.Size([3, 24])
	Multihead-key size = torch.Size([3, 24])
	Multihead-value size = torch.Size([3, 28])
</code></pre>
<p>But for extend this example to all the words, we have to use the <code>bmm()</code> (Batch Matrix Mutliplication) method from <strong>PyTorch</strong>. It&rsquo;s useful when there&rsquo;s the need to deal with tensor. The effective <code>matmul</code>is performed along the last two dimensions, while the first one representing heads is preserved.</p>
<p>Finally, to make the input feasible (i.e., choose the right dimensions) for the multi-head attention layer, I need to replicate it for the number of attention heads (in this case, three times).</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="n">repeated_inputs</span> <span class="o">=</span> <span class="n">embedded_sentence</span><span class="o">.</span><span class="n">T</span><span class="o">.</span><span class="n">repeat</span><span class="p">(</span><span class="n">n_heads</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Repeated input size =&#39;</span><span class="p">,</span> <span class="n">repeated_inputs</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
</span></span></code></pre></div><pre><code>Repeated input size = torch.Size([3, 16, 8])
</code></pre>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="n">multihead_queries</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">bmm</span><span class="p">(</span><span class="n">multihead_W_query</span><span class="p">,</span> <span class="n">repeated_inputs</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">multihead_keys</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">bmm</span><span class="p">(</span><span class="n">multihead_W_key</span><span class="p">,</span> <span class="n">repeated_inputs</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">multihead_values</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">bmm</span><span class="p">(</span><span class="n">multihead_W_value</span><span class="p">,</span> <span class="n">repeated_inputs</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="s1">&#39;All input words -&gt; multihead_queries.shape =&#39;</span><span class="p">,</span> <span class="n">multihead_queries</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span> 
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="s1">&#39;All input words -&gt; multihead_keys.shape =&#39;</span><span class="p">,</span> <span class="n">multihead_keys</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span> 
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="s1">&#39;All input words -&gt; multihead_values.shape =&#39;</span><span class="p">,</span> <span class="n">multihead_values</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
</span></span></code></pre></div><pre><code>All input words -&gt; multihead_queries.shape = torch.Size([3, 24, 8])
All input words -&gt; multihead_keys.shape = torch.Size([3, 24, 8])
All input words -&gt; multihead_values.shape = torch.Size([3, 28, 8])
</code></pre>
<p>But - as can be seen from above - the last two dimensions are swapped, because I would like to have words as the second dimension and vector components as third one. Hence, let&rsquo;s permute (i.e., swap) the last two dimensions.</p>
<p>Here, the <code>permute()</code> method is asking the desidered ordering of dimensions.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="n">multihead_queries</span> <span class="o">=</span> <span class="n">multihead_queries</span><span class="o">.</span><span class="n">permute</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">multihead_keys</span> <span class="o">=</span> <span class="n">multihead_keys</span><span class="o">.</span><span class="n">permute</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">multihead_values</span> <span class="o">=</span> <span class="n">multihead_values</span><span class="o">.</span><span class="n">permute</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="s1">&#39;All input words -&gt; multihead_queries.shape =&#39;</span><span class="p">,</span> <span class="n">multihead_queries</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span> 
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="s1">&#39;All input words -&gt; multihead_keys.shape =&#39;</span><span class="p">,</span> <span class="n">multihead_keys</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span> 
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="s1">&#39;All input words -&gt; multihead_values.shape =&#39;</span><span class="p">,</span> <span class="n">multihead_values</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
</span></span></code></pre></div><pre><code>All input words -&gt; multihead_queries.shape = torch.Size([3, 8, 24])
All input words -&gt; multihead_keys.shape = torch.Size([3, 8, 24])
All input words -&gt; multihead_values.shape = torch.Size([3, 8, 28])
</code></pre>
<h2 id="compute-unnormalized-attention-scores">Compute unnormalized attention scores</h2>
<p>As did before, compute the unnormalzied attention scores for each head.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="n">multihead_omega_all</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">bmm</span><span class="p">(</span><span class="n">multihead_queries</span><span class="p">,</span> <span class="n">multihead_keys</span><span class="o">.</span><span class="n">permute</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">1</span><span class="p">))</span>
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;Unnormalized attention scores of ALL words w.r.t. ALL other words:</span><span class="se">\n</span><span class="si">{</span><span class="n">multihead_omega_all</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="s2">&#34;All input words scores -&gt; omega_all.shape =&#34;</span><span class="p">,</span> <span class="n">multihead_omega_all</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span> 
</span></span></code></pre></div><pre><code>Unnormalized attention scores of ALL words w.r.t. ALL other words:
tensor([[[  63.3466,   69.7982,   60.7684,  168.7238,  -35.8708,  -90.1632,
            69.7982,   63.3466],
         [  64.3794,   50.0433,   56.6445,  156.4643,  -33.4781,  -78.7985,
            50.0433,   64.3794],
         [  54.1203,   63.1504,   53.7396,  154.9160,  -35.4911,  -82.0983,
            63.1504,   54.1203],
         [ 178.0788,  180.1088,  162.0738,  483.6229,  -87.1213, -246.8582,
           180.1088,  178.0788],
         [ -27.7026,  -43.6290,  -35.4398, -103.1395,   30.3216,   52.3912,
           -43.6290,  -27.7026],
         [ -76.8325,  -87.6214,  -66.1114, -224.5666,   31.7686,  111.7955,
           -87.6214,  -76.8325],
         [  64.3794,   50.0433,   56.6445,  156.4643,  -33.4781,  -78.7985,
            50.0433,   64.3794],
         [  63.3466,   69.7982,   60.7684,  168.7238,  -35.8708,  -90.1632,
            69.7982,   63.3466]],

        [[  40.2509,   51.2320,   37.3632,  140.1730,  -24.4904,  -60.2304,
            51.2320,   40.2509],
         [  54.8854,   63.9741,   42.0731,  159.9941,  -37.8272,  -62.2964,
            63.9741,   54.8854],
         [  45.0314,   58.8531,   46.1473,  171.9726,  -39.9998,  -75.0010,
            58.8531,   45.0314],
         [ 147.8137,  211.9995,  141.7101,  559.7228, -125.7353, -237.5117,
           211.9995,  147.8137],
         [ -36.2802,  -40.3992,  -36.5809, -128.0536,   31.7833,   45.6996,
           -40.3992,  -36.2802],
         [ -70.0388, -124.8755,  -72.3371, -282.8608,   65.5955,  124.3698,
          -124.8755,  -70.0388],
         [  54.8854,   63.9741,   42.0731,  159.9941,  -37.8272,  -62.2964,
            63.9741,   54.8854],
         [  40.2509,   51.2320,   37.3632,  140.1730,  -24.4904,  -60.2304,
            51.2320,   40.2509]],

        [[  62.2692,   34.5424,   52.8352,  160.1184,  -40.1587,  -68.9696,
            34.5424,   62.2692],
         [  72.3984,   51.9031,   57.8771,  202.6503,  -44.1590, -101.3688,
            51.9031,   72.3984],
         [  52.9430,   40.0645,   43.6347,  147.3360,  -31.0522,  -61.0057,
            40.0645,   52.9430],
         [ 139.6398,  107.2641,  127.2303,  425.1722,  -99.6243, -196.1776,
           107.2641,  139.6398],
         [ -27.6845,  -25.2708,  -27.7060,  -81.5897,   25.3154,   31.5857,
           -25.2708,  -27.6845],
         [ -83.8774,  -66.2358,  -70.5194, -238.9998,   54.5963,  106.6016,
           -66.2358,  -83.8774],
         [  72.3984,   51.9031,   57.8771,  202.6503,  -44.1590, -101.3688,
            51.9031,   72.3984],
         [  62.2692,   34.5424,   52.8352,  160.1184,  -40.1587,  -68.9696,
            34.5424,   62.2692]]])
All input words scores -&gt; omega_all.shape = torch.Size([3, 8, 8])
</code></pre>
<h3 id="compute-normalized-attention-scores-for-all-input-words-1">Compute normalized attention scores for ALL input words</h3>
<p>Let&rsquo;s compute the normalized attention score \(\alpha\) (alpha) for ALL words w.r.t. ALL other words.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="n">multihead_normalized_attention_scores</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">multihead_omega_all</span> <span class="o">/</span> <span class="n">dim_k</span> <span class="o">**</span> <span class="mf">0.5</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;Multihead normalized attention scores of ALL words w.r.t. ALL other words:</span><span class="se">\n</span><span class="si">{</span><span class="n">multihead_normalized_attention_scores</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Sum of this vector =&#39;</span><span class="p">,</span> <span class="n">multihead_normalized_attention_scores</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span><span class="o">.</span><span class="n">item</span><span class="p">())</span>
</span></span></code></pre></div><pre><code>Multihead normalized attention scores of ALL words w.r.t. ALL other words:
tensor([[[5.5202e-01, 9.7718e-01, 8.2888e-01, 8.5065e-01, 8.6035e-02,
          1.8976e-03, 9.7718e-01, 5.5202e-01],
         [1.5916e-01, 5.0917e-02, 4.2787e-01, 8.0445e-05, 6.5592e-01,
          3.3284e-02, 5.0917e-02, 1.5916e-01],
         [5.1472e-01, 7.0178e-01, 7.4659e-01, 2.9650e-02, 2.5820e-01,
          1.2600e-02, 7.0178e-01, 5.1472e-01],
         [9.9754e-01, 1.4868e-03, 9.8379e-01, 1.7937e-07, 9.2739e-01,
          3.2141e-05, 1.4868e-03, 9.9754e-01],
         [4.5928e-01, 2.2055e-02, 1.5059e-01, 1.2142e-02, 3.6933e-01,
          7.8774e-01, 2.2055e-02, 4.5928e-01],
         [1.9087e-01, 1.2551e-02, 5.9268e-01, 9.5008e-01, 9.0600e-04,
          6.9592e-02, 1.2551e-02, 1.9087e-01],
         [1.5916e-01, 5.0917e-02, 4.2787e-01, 8.0445e-05, 6.5592e-01,
          3.3284e-02, 5.0917e-02, 1.5916e-01],
         [5.5202e-01, 9.7718e-01, 8.2888e-01, 8.5065e-01, 8.6035e-02,
          1.8976e-03, 9.7718e-01, 5.5202e-01]],

        [[4.9490e-03, 2.2084e-02, 6.9762e-03, 2.5045e-03, 8.7811e-01,
          8.5455e-01, 2.2084e-02, 4.9490e-03],
         [2.2918e-02, 8.7465e-01, 2.1855e-02, 1.6536e-04, 2.6996e-01,
          9.6638e-01, 8.7465e-01, 2.2918e-02],
         [8.0509e-02, 2.9191e-01, 1.5850e-01, 9.6404e-01, 1.0286e-01,
          5.3648e-02, 2.9191e-01, 8.0509e-02],
         [2.0698e-03, 9.9851e-01, 1.5405e-02, 1.0000e+00, 3.5005e-04,
          2.1658e-04, 9.9851e-01, 2.0698e-03],
         [7.9740e-02, 4.2640e-02, 1.1930e-01, 7.5100e-05, 4.9774e-01,
          2.0099e-01, 4.2640e-02, 7.9740e-02],
         [7.6382e-01, 6.2532e-06, 1.6631e-01, 6.4562e-06, 9.0342e-01,
          9.0630e-01, 6.2532e-06, 7.6382e-01],
         [2.2918e-02, 8.7465e-01, 2.1855e-02, 1.6536e-04, 2.6996e-01,
          9.6638e-01, 8.7465e-01, 2.2918e-02],
         [4.9490e-03, 2.2084e-02, 6.9762e-03, 2.5045e-03, 8.7811e-01,
          8.5455e-01, 2.2084e-02, 4.9490e-03]],

        [[4.4303e-01, 7.3207e-04, 1.6414e-01, 1.4685e-01, 3.5855e-02,
          1.4355e-01, 7.3207e-04, 4.4303e-01],
         [8.1792e-01, 7.4429e-02, 5.5028e-01, 9.9975e-01, 7.4127e-02,
          3.3219e-04, 7.4429e-02, 8.1792e-01],
         [4.0477e-01, 6.3044e-03, 9.4906e-02, 6.3105e-03, 6.3894e-01,
          9.3375e-01, 6.3044e-03, 4.0477e-01],
         [3.9022e-04, 5.1828e-10, 8.0173e-04, 1.1806e-12, 7.2256e-02,
          9.9975e-01, 5.1828e-10, 3.9022e-04],
         [4.6098e-01, 9.3530e-01, 7.3012e-01, 9.8778e-01, 1.3293e-01,
          1.1271e-02, 9.3530e-01, 4.6098e-01],
         [4.5311e-02, 9.8744e-01, 2.4102e-01, 4.9918e-02, 9.5677e-02,
          2.4106e-02, 9.8744e-01, 4.5311e-02],
         [8.1792e-01, 7.4429e-02, 5.5028e-01, 9.9975e-01, 7.4127e-02,
          3.3219e-04, 7.4429e-02, 8.1792e-01],
         [4.4303e-01, 7.3207e-04, 1.6414e-01, 1.4685e-01, 3.5855e-02,
          1.4355e-01, 7.3207e-04, 4.4303e-01]]])
Sum of this vector = 64.0
</code></pre>
<h3 id="compute-z-for-all-input-words-1">Compute \(z\) for ALL input words</h3>
<p>Let&rsquo;s compute \(z\) for ALL words w.r.t. ALL other words.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="n">multihead_context_z</span> <span class="o">=</span> <span class="n">multihead_normalized_attention_scores</span> <span class="o">@</span> <span class="n">multihead_values</span>
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Multihead context vector related to ALL input words w.r.t. ALL other words&#39;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Shape =&#39;</span><span class="p">,</span> <span class="n">multihead_context_z</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="n">multihead_context_z</span><span class="p">)</span>
</span></span></code></pre></div><pre><code>Multihead context vector related to ALL input words w.r.t. ALL other words
Shape = torch.Size([3, 8, 28])
tensor([[[ 1.2550e+01,  8.2323e+00,  9.1246e+00,  2.9574e+00,  1.3149e+01,
           1.1474e+01,  5.4783e+00,  8.9908e+00,  8.1721e+00,  9.5334e+00,
           1.2736e+01,  1.0823e+01,  1.4178e+01,  1.3128e+01,  1.0271e+01,
           7.1355e+00,  9.9221e+00,  1.1401e+01,  9.2407e+00,  8.2230e+00,
           1.4161e+01,  1.2087e+01,  5.3284e+00,  1.3517e+01,  1.0876e+01,
           1.0542e+01,  1.3312e+01,  9.9359e+00],
         [ 8.4613e-01,  5.4901e-01, -9.3701e-01, -5.6394e-02,  1.7809e-01,
           6.3644e-01,  3.3348e-01, -6.3719e-02,  7.8778e-01,  4.3655e-01,
           5.3653e-01,  6.2437e-01,  7.7565e-01, -5.2384e-01, -1.3249e-01,
           5.0235e-01,  1.7263e-01,  9.4807e-01, -1.7731e-01,  8.8894e-02,
           9.8492e-02,  6.2525e-01, -1.3491e+00,  1.0078e+00,  5.1598e-01,
          -1.0351e+00,  7.7803e-01,  1.1364e+00],
         [ 6.7393e+00,  4.8771e+00,  3.2697e+00,  4.0000e-01,  6.6771e+00,
           6.5647e+00,  1.8090e+00,  4.0669e+00,  2.9440e+00,  4.5260e+00,
           6.0300e+00,  5.4758e+00,  8.6360e+00,  6.0929e+00,  4.9760e+00,
           3.3591e+00,  4.7602e+00,  6.6984e+00,  3.2040e+00,  4.1491e+00,
           7.3087e+00,  6.6017e+00, -3.4923e-01,  7.0143e+00,  4.2075e+00,
           4.8420e+00,  6.6773e+00,  5.5571e+00],
         [ 2.7140e+00,  4.7744e+00,  1.2109e+00,  4.0982e+00,  5.1366e+00,
           4.2388e+00,  2.9954e+00,  4.2085e+00,  1.1205e+00,  5.9954e-01,
           3.1774e-01,  7.3186e+00,  6.3972e+00,  1.9380e+00,  5.1665e+00,
           1.1845e+00,  4.1889e+00,  5.8980e+00,  3.1623e+00,  4.1146e+00,
           4.7828e+00,  3.5414e+00, -2.6044e+00,  3.4029e+00,  3.0947e+00,
           2.5684e+00,  1.9649e+00,  3.0005e+00],
         [-1.9108e+00, -7.5741e-01, -1.6170e+00, -4.9262e-01, -1.3816e-01,
          -1.6101e+00, -2.3048e-01,  1.2725e+00, -2.9797e+00, -3.0562e+00,
          -1.5322e+00,  1.6294e+00,  1.2837e+00,  1.6719e-01, -1.0773e+00,
          -1.5306e+00, -8.8325e-01,  1.2672e+00, -1.8476e+00,  1.0466e+00,
          -7.4326e-01, -1.3275e+00, -4.3374e+00,  7.7009e-01, -4.2708e-01,
          -8.8085e-01, -1.6558e+00,  1.1952e+00],
         [ 4.7833e+00,  4.2254e+00,  6.5103e+00,  4.6408e+00,  7.3923e+00,
           4.5949e+00,  5.0826e+00,  6.4071e+00,  4.6354e+00,  4.2027e+00,
           5.3380e+00,  7.5981e+00,  6.1271e+00,  6.7390e+00,  6.8821e+00,
           3.3658e+00,  6.1592e+00,  5.8023e+00,  7.0229e+00,  5.0363e+00,
           7.9460e+00,  5.2522e+00,  5.7094e+00,  6.2701e+00,  7.7611e+00,
           6.6257e+00,  5.8532e+00,  4.3290e+00],
         [ 8.4613e-01,  5.4901e-01, -9.3701e-01, -5.6395e-02,  1.7809e-01,
           6.3644e-01,  3.3348e-01, -6.3719e-02,  7.8778e-01,  4.3655e-01,
           5.3653e-01,  6.2437e-01,  7.7565e-01, -5.2384e-01, -1.3249e-01,
           5.0235e-01,  1.7263e-01,  9.4807e-01, -1.7731e-01,  8.8894e-02,
           9.8493e-02,  6.2525e-01, -1.3491e+00,  1.0078e+00,  5.1598e-01,
          -1.0351e+00,  7.7803e-01,  1.1364e+00],
         [ 1.2550e+01,  8.2323e+00,  9.1246e+00,  2.9574e+00,  1.3149e+01,
           1.1474e+01,  5.4783e+00,  8.9908e+00,  8.1721e+00,  9.5334e+00,
           1.2736e+01,  1.0823e+01,  1.4178e+01,  1.3128e+01,  1.0271e+01,
           7.1355e+00,  9.9221e+00,  1.1401e+01,  9.2407e+00,  8.2230e+00,
           1.4161e+01,  1.2087e+01,  5.3284e+00,  1.3517e+01,  1.0876e+01,
           1.0542e+01,  1.3312e+01,  9.9359e+00]],

        [[-3.0829e+00, -4.3344e+00, -3.8598e+00, -2.1292e+00, -2.6663e+00,
          -1.8364e+00,  2.5687e-01, -6.3279e+00, -3.2069e+00, -3.2803e+00,
          -1.0921e+00, -4.5912e+00, -6.8437e-01,  3.7830e-01, -3.6839e-01,
          -2.0829e+00, -2.6025e+00, -4.2082e+00, -4.3396e+00, -2.9734e+00,
          -2.1751e+00, -3.2557e+00, -9.7399e-01, -1.9781e+00, -1.0855e+00,
          -1.8056e+00, -2.5949e+00, -4.7041e+00],
         [ 3.0845e+00, -9.0204e-01, -3.8693e+00,  4.5435e-03,  1.5541e-01,
          -2.5049e+00,  4.0231e+00, -6.7520e+00, -1.5541e+00, -5.8121e-01,
           2.5355e+00, -2.8262e+00,  1.5110e+00,  2.3968e+00,  5.0290e+00,
          -7.1024e-02, -1.2569e+00, -1.7446e+00, -1.8978e+00, -1.9663e-02,
           1.5819e+00, -2.5593e+00,  4.8311e-01, -1.4362e+00,  8.9585e-01,
           2.2621e-01, -6.3192e-01,  9.6575e-01],
         [ 5.2792e+00,  5.8114e+00,  3.1216e+00,  4.8434e+00,  6.1506e+00,
           4.7512e+00,  6.1214e+00,  5.1191e+00,  3.7631e+00,  5.7305e+00,
           6.5150e+00,  4.5658e+00,  4.9792e+00,  4.2492e+00,  7.3722e+00,
           6.2548e+00,  3.7377e+00,  4.4937e+00,  5.0615e+00,  5.5706e+00,
           6.0054e+00,  5.2596e+00,  4.5952e+00,  3.2967e+00,  4.7211e+00,
           4.6390e+00,  4.1446e+00,  6.8238e+00],
         [ 9.5750e+00,  7.7102e+00,  2.6211e+00,  6.1118e+00,  9.5336e+00,
           3.7211e+00,  9.6168e+00,  3.6955e+00,  3.9494e+00,  7.2018e+00,
           9.3044e+00,  5.1696e+00,  6.6733e+00,  5.4449e+00,  1.1458e+01,
           5.9527e+00,  4.9379e+00,  5.7139e+00,  6.2403e+00,  6.1889e+00,
           9.8552e+00,  4.8115e+00,  4.9289e+00,  3.1193e+00,  5.1594e+00,
           4.8541e+00,  6.3040e+00,  9.4579e+00],
         [-5.6094e-01, -1.1267e+00, -8.5515e-01, -5.0801e-01, -3.0618e-01,
           2.7183e-02,  6.2272e-01, -1.9285e+00, -7.3523e-01, -8.9529e-01,
           2.9595e-02, -1.2084e+00,  4.0297e-01,  9.4168e-01,  4.5556e-01,
          -5.4734e-01, -1.0261e-01, -1.3311e+00, -1.2880e+00, -7.8507e-01,
          -7.4631e-01, -7.8153e-01, -7.5138e-02, -2.1338e-01, -1.2993e-01,
          -8.4351e-01, -3.6570e-01, -1.5080e+00],
         [-7.4788e-01, -1.7038e+00, -1.7669e+00, -2.7876e-01, -2.6155e+00,
           2.2724e+00,  1.1839e+00, -3.0407e+00,  8.9993e-01, -1.0570e+00,
          -1.9050e-01, -2.6671e+00,  1.6860e+00,  4.7182e+00,  3.1012e+00,
           3.2457e+00, -2.4241e-01, -3.0754e+00, -2.5329e+00,  1.5982e+00,
          -5.2954e+00, -6.2323e-01,  1.4935e+00,  1.3279e+00,  3.0294e+00,
           9.1379e-01, -2.9769e+00, -5.4288e-01],
         [ 3.0845e+00, -9.0205e-01, -3.8693e+00,  4.5432e-03,  1.5541e-01,
          -2.5049e+00,  4.0231e+00, -6.7520e+00, -1.5541e+00, -5.8121e-01,
           2.5355e+00, -2.8262e+00,  1.5110e+00,  2.3968e+00,  5.0290e+00,
          -7.1024e-02, -1.2569e+00, -1.7446e+00, -1.8978e+00, -1.9663e-02,
           1.5819e+00, -2.5593e+00,  4.8311e-01, -1.4362e+00,  8.9585e-01,
           2.2621e-01, -6.3192e-01,  9.6575e-01],
         [-3.0829e+00, -4.3344e+00, -3.8598e+00, -2.1292e+00, -2.6663e+00,
          -1.8364e+00,  2.5687e-01, -6.3279e+00, -3.2069e+00, -3.2803e+00,
          -1.0921e+00, -4.5912e+00, -6.8437e-01,  3.7830e-01, -3.6839e-01,
          -2.0829e+00, -2.6025e+00, -4.2082e+00, -4.3396e+00, -2.9734e+00,
          -2.1751e+00, -3.2557e+00, -9.7399e-01, -1.9781e+00, -1.0855e+00,
          -1.8056e+00, -2.5949e+00, -4.7041e+00]],

        [[ 1.7277e+00,  2.7949e+00,  1.8540e+00,  3.2673e+00,  1.2106e+00,
           2.6408e+00,  2.5985e+00,  1.7675e-01,  7.6661e-01,  2.8296e+00,
           1.9629e+00,  1.0371e+00,  1.0960e+00,  1.8360e+00,  2.4581e+00,
           2.3240e+00,  3.4276e+00,  4.6789e-01,  2.7870e+00,  3.5721e+00,
           1.4413e+00,  1.9314e-01,  5.0585e-01,  6.4904e-01,  1.0730e+00,
           2.4695e+00,  6.4522e-01,  1.0604e+00],
         [ 8.0561e+00,  8.7068e+00,  8.0398e+00,  1.0292e+01,  6.3139e+00,
           1.0025e+01,  1.0486e+01,  5.4967e+00,  6.5935e+00,  9.6562e+00,
           7.9706e+00,  8.0124e+00,  4.8594e+00,  7.4863e+00,  9.0422e+00,
           8.2225e+00,  1.1548e+01,  6.4927e+00,  9.2785e+00,  1.1288e+01,
           7.9983e+00,  5.3607e+00,  6.8151e+00,  5.8418e+00,  6.7265e+00,
           1.0034e+01,  7.0275e+00,  6.3063e+00],
         [ 1.3572e-02, -1.9507e+00, -2.8280e+00, -3.7075e-01, -3.3975e+00,
          -1.0711e+00, -3.5136e-01, -3.3840e+00, -2.7964e+00, -2.3936e+00,
           9.7438e-01, -3.0568e+00,  4.0147e-01, -1.8839e+00, -8.7062e-01,
           1.2632e+00, -2.3820e-02, -3.5027e+00, -7.6090e-01, -8.2943e-01,
          -1.7677e+00, -4.5607e+00, -3.8958e+00, -3.5944e+00, -7.7866e-01,
          -7.4185e-02, -3.6162e+00, -5.5365e-01],
         [-2.2992e+00, -4.0919e+00, -3.2351e+00, -1.7475e+00, -4.0110e+00,
          -2.2393e+00, -2.7416e+00, -3.0424e+00, -3.3113e+00, -3.5505e+00,
          -2.9581e-02, -2.9735e+00, -1.4496e+00, -3.4591e+00, -2.4393e+00,
          -1.0472e+00, -1.4362e+00, -2.4217e+00, -2.8877e+00, -9.7912e-01,
          -2.4756e+00, -4.1629e+00, -4.8334e+00, -4.0344e+00, -1.9015e+00,
          -1.7138e+00, -2.9184e+00, -1.3584e+00],
         [ 1.3143e+01,  7.6980e+00,  4.4144e+00,  9.7790e+00,  5.7849e+00,
           1.3962e+01,  1.3737e+01,  9.1561e+00,  1.0076e+01,  6.7784e+00,
           1.1504e+01,  1.2343e+01,  6.4229e+00,  1.0729e+01,  6.2264e+00,
           1.1082e+01,  1.2218e+01,  7.7111e+00,  1.0279e+01,  1.0411e+01,
           1.1958e+01,  7.6079e+00,  1.0849e+01,  5.2756e+00,  7.2323e+00,
           1.5426e+01,  1.4528e+01,  1.0123e+01],
         [ 7.4917e+00,  1.7766e+00, -2.4720e+00,  2.3755e+00,  6.9855e-01,
           6.8079e+00,  6.0625e+00,  4.1528e+00,  4.5432e+00, -6.5834e-01,
           5.4717e+00,  5.7336e+00,  2.7809e+00,  5.6617e+00, -1.0012e+00,
           5.2573e+00,  3.6116e+00,  1.5208e+00,  3.8611e+00,  1.9517e+00,
           5.8181e+00,  2.8174e+00,  5.1923e+00,  4.8661e-03,  1.3811e+00,
           8.2654e+00,  8.8761e+00,  5.0839e+00],
         [ 8.0561e+00,  8.7068e+00,  8.0398e+00,  1.0292e+01,  6.3139e+00,
           1.0025e+01,  1.0486e+01,  5.4967e+00,  6.5935e+00,  9.6562e+00,
           7.9706e+00,  8.0124e+00,  4.8594e+00,  7.4863e+00,  9.0422e+00,
           8.2225e+00,  1.1548e+01,  6.4927e+00,  9.2785e+00,  1.1288e+01,
           7.9983e+00,  5.3607e+00,  6.8151e+00,  5.8418e+00,  6.7265e+00,
           1.0034e+01,  7.0275e+00,  6.3063e+00],
         [ 1.7277e+00,  2.7949e+00,  1.8540e+00,  3.2673e+00,  1.2106e+00,
           2.6408e+00,  2.5985e+00,  1.7675e-01,  7.6661e-01,  2.8296e+00,
           1.9629e+00,  1.0371e+00,  1.0960e+00,  1.8360e+00,  2.4581e+00,
           2.3240e+00,  3.4276e+00,  4.6789e-01,  2.7870e+00,  3.5721e+00,
           1.4413e+00,  1.9314e-01,  5.0585e-01,  6.4904e-01,  1.0730e+00,
           2.4695e+00,  6.4522e-01,  1.0604e+00]]])
</code></pre>
<h1 id="cross-attention">Cross-attention</h1>
<p>So, that&rsquo;s it for this post. The next one will be focusing on the cross-attention, the mechanism used in Transformers to perform an attention calculus among different inputs.</p>
</div>
    <div class="post__footer">
      

      
        <span><a class="tag" href="/it/tags/attention/">attention</a><a class="tag" href="/it/tags/nlp/">nlp</a><a class="tag" href="/it/tags/deep-learning/">deep learning</a></span>


      
    </div>

    <div id="comment">
          <h2>commenti</h2>
          <div id="disqus_thread"></div>
<script type="application/javascript">
    window.disqus_config = function () {
    
    
    
    };
    (function() {
        if (["localhost", "127.0.0.1"].indexOf(window.location.hostname) != -1) {
            document.getElementById('disqus_thread').innerHTML = 'Disqus comments not available by default when the website is previewed locally.';
            return;
        }
        var d = document, s = d.createElement('script'); s.async = true;
        s.src = '//' + "lorenzobalzani" + '.disqus.com/embed.js';
        s.setAttribute('data-timestamp', +new Date());
        (d.head || d.body).appendChild(s);
    })();
</script>
<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
<a href="https://disqus.com" class="dsq-brlink">comments powered by <span class="logo-disqus">Disqus</span></a>
        </div>
  </div>

      </main>
    </div><footer class="footer footer__base">
  <ul class="footer__list">
    <li class="footer__item">
      &copy;
      
        Lorenzo Balzani
        2024
      
    </li>
    
  </ul>
</footer>
  
  <script
    type="text/javascript"
    src="/js/medium-zoom.min.1248fa75275e5ef0cbef27e8c1e27dc507c445ae3a2c7d2ed0be0809555dac64.js"
    integrity="sha256-Ekj6dSdeXvDL7yfoweJ9xQfERa46LH0u0L4ICVVdrGQ="
    crossorigin="anonymous"
  ></script><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.13.0/dist/katex.min.css" integrity="sha384-t5CR&#43;zwDAROtph0PXGte6ia8heboACF9R5l/DiY&#43;WZ3P2lxNgvJkQk5n7GPvLMYw" crossorigin="anonymous" /><script defer src="https://cdn.jsdelivr.net/npm/katex@0.13.0/dist/katex.min.js" integrity="sha384-FaFLTlohFghEIZkw6VGwmf9ISTubWAVYW8tG8&#43;w2LAIftJEULZABrF9PPFv&#43;tVkH" crossorigin="anonymous"></script><script
      defer
      src="https://cdn.jsdelivr.net/npm/katex@0.13.0/dist/contrib/auto-render.min.js"
      integrity="sha384-bHBqxz8fokvgoJ/sc17HODNxa42TlaEhB&#43;w8ZJXTc2nZf1VgEaFZeZvT4Mznfz0v"
      crossorigin="anonymous"
      onload="renderMathInElement(document.body);"
    ></script></body>
</html>
